{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6f524-bb53-4b95-9b76-e87db0ffb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAAE trained on celebA faces\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from function import prep_data, make_new_folder, plot_losses, save_input_args, shift_x, plot_norm_losses\n",
    "from dataload import CELEBA \n",
    "from models import DAE, DIS_Z, LINEAR_SVM\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import argparse\n",
    "\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6591c-5a72-4439-b2c8-c900507a3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.root = '../data'\n",
    "        self.batchSize = 64\n",
    "        self.maxEpochs = 100\n",
    "        self.nz = 200\n",
    "        self.lr = 1e-4\n",
    "        self.fSize = 64\n",
    "        self.outDir = '../Experiments/DAAE1000/'\n",
    "        self.commit = 'xxx'\n",
    "        self.alpha = 1.0\n",
    "        self.sigma = 0.1\n",
    "        self.M = 5\n",
    "        self.loss = 'BCE' #'BCE'\n",
    "        self.loadDAE = False\n",
    "        self.loadSVM = False\n",
    "        self.load_DAE_from = None\n",
    "        self.evalMode = False\n",
    "        self.comment = ''\n",
    "        self.momentum = 0.1\n",
    "        self.c = 0.01\n",
    "        self.svmLR = 1e-4\n",
    "        self.Ntest = 100\n",
    "        self.gpuNo = 3\n",
    "        self.multimodalZ = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533744f3-4635-4420-b6d0-e9c627a00c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--root', default='/data', type=str) #data/datasets/LabelSwap\n",
    "#     parser.add_argument('--batchSize', default=64, type=int)\n",
    "#     parser.add_argument('--maxEpochs', default=10, type=int)\n",
    "#     parser.add_argument('--nz', default=200, type=int)\n",
    "#     parser.add_argument('--lr', default=1e-3, type=float)\n",
    "#     parser.add_argument('--fSize', default=64, type=int)  # multiple of filters to use\n",
    "#     parser.add_argument('--outDir', default='../../Experiments/DAAE1000/', type=str)\n",
    "#     # parser.add_argument('--commit', required=True, type=str)\n",
    "#     parser.add_argument('--commit', default='xxx', type=str)\n",
    "\n",
    "#     parser.add_argument('--alpha', default=1.0, type=float)  # weight on the adversarial cost\n",
    "#     parser.add_argument('--sigma', default=0.1, type=float)  # noise level\n",
    "#     parser.add_argument('--M', default=5, type=int)  #number of sampling iterations\n",
    "#     parser.add_argument('--loss', default='BCE', type=str) #'BCE' or 'MSE' currently supported\n",
    "#     parser.add_argument('--loadDAE', action='store_true')\n",
    "#     parser.add_argument('--loadSVM', action='store_true')\n",
    "#     parser.add_argument('--load_DAE_from', default=None, type=str)\n",
    "#     parser.add_argument('--evalMode', action='store_true')\n",
    "#     parser.add_argument('--comment', type=str)\n",
    "#     parser.add_argument('--momentum', default=0.9, type=float) \n",
    "#     parser.add_argument('--c', type=float, default=0.01) #for training the linearSVM for eval\n",
    "#     parser.add_argument('--svmLR', type=float, default=1e-4)\n",
    "\n",
    "#     parser.add_argument('--Ntest', default=100, type=int)\n",
    "#     # parser.add_argument('--gpuNo', required=True, type=int)\n",
    "#     parser.add_argument('--gpuNo', default=0, type=int)\n",
    "\n",
    "\n",
    "#     return parser.parse_args()\n",
    "\n",
    "def build_dis(dae, multimodalZ):\n",
    "    if not multimodalZ:\n",
    "        print('\\n ** USING NORMAL PRIOR **')\n",
    "        prior = dae.norm_prior\n",
    "        NZ = opts.nz\n",
    "    else:\n",
    "        print('\\n ** USING MULTIMODAL PRIOR **')\n",
    "        prior = dae.multi_prior\n",
    "        NZ = 2\n",
    "    dis = DIS_Z(nz=NZ, prior=prior)\n",
    "\n",
    "    return dis, NZ\n",
    "\n",
    "def svm_score(svm, y, x=None, enc=None, dae=None):\n",
    "    '''\n",
    "    EITHER\n",
    "        take a data sample x AND a dae\n",
    "    OR\n",
    "        take an encoding\n",
    "    and apply SVM and get score\n",
    "\n",
    "    '''\n",
    "    assert (x is not None) or (enc is not None)\n",
    "    if enc is None:\n",
    "        assert dae is not None\n",
    "        enc = dae.encode(x)\n",
    "    output = svm.forward(enc)\n",
    "    score = svm.binary_class_score(output, y)\n",
    "    return score\n",
    "\n",
    "def eval_mode(dae, exDir, M, testLoader, svm=None):\n",
    "    f = open(join(exDir, 'outputs.txt'), 'w')\n",
    "\n",
    "    ## reconstruction error ##\n",
    "    recError = []\n",
    "    print('calculating reconstruction error...')\n",
    "    for i, data in enumerate(testLoader):\n",
    "        x, y = prep_data(data, useCUDA=dae.useCUDA)\n",
    "        zTest = dae.encode(x)\n",
    "        recTest = dae.decode(zTest)\n",
    "        # recError.append(dae.rec_loss(recTest, x).data[0])\n",
    "        recError.append(dae.rec_loss(recTest, x).detach().cpu().numpy())\n",
    "    meanRecError = np.mean(recError)\n",
    "    f.write('mean reconstruction error (non-corrupted): %0.5f' % (meanRecError))\n",
    "\n",
    "\n",
    "    #eval samples ##TODO\n",
    "\n",
    "    ## representation robustness (shift) ##\n",
    "    print('performing robustness plot...')\n",
    "    maxShift = x.size(2)//2\n",
    "    maxShift_1 = x.size(3)//2\n",
    "    step = 4\n",
    "    axis = range(-maxShift, maxShift, step)\n",
    "    axis_1 = range(-maxShift_1, maxShift_1, step)\n",
    "    if x.size(2)%2 == 0:\n",
    "        dim_x = maxShift*2//step\n",
    "    else:\n",
    "        dim_x = maxShift*2//step+1\n",
    "    if x.size(3)%2 == 0:\n",
    "        dim_y = maxShift_1*2//step\n",
    "    else:\n",
    "        dim_y = maxShift_1*2//step+1\n",
    "    \n",
    "    robustnessMap = torch.Tensor(dim_x, dim_y).fill_(0)\n",
    "    classMap = torch.Tensor(dim_x, dim_y).fill_(0)\n",
    "    x, y = prep_data(iter(testLoader).next(), useCUDA=dae.useCUDA)  #take a batch of samples\n",
    "    allShifts=[]\n",
    "    enc00 = dae.encode(x)\n",
    "    for j, dx in enumerate(axis):\n",
    "        for i, dy in enumerate(axis_1):\n",
    "            xShift = shift_x(x, dy, dx)\n",
    "            encDxDy = dae.encode(xShift)\n",
    "            # diff = [(torch.dot(encDxDy[k], enc00[k])/ (torch.norm(encDxDy[k])*torch.norm(enc00[k]))).data[0] for k in range(encDxDy.size(0))]\n",
    "            # diff = [torch.dot(encDxDy[k], enc00[k]).data[0]/ ((torch.norm(encDxDy[k])*torch.norm(enc00[k])).data[0] + 1e-6) for k in range(encDxDy.size(0))]\n",
    "            diff = [torch.dot(encDxDy[k], enc00[k]).detach().cpu().numpy()/ ((torch.norm(encDxDy[k])*torch.norm(enc00[k])).detach().cpu().numpy() + 1e-6) for k in range(encDxDy.size(0))]\n",
    "            robustnessMap[j,i] = np.mean(diff)\n",
    "            # classMap[j,i] = svm_score(svm, y, enc=encDxDy).data[0]\n",
    "            classMap[j,i] = svm_score(svm, y, enc=encDxDy).detach().cpu()\n",
    "\n",
    "            allShifts.append(xShift[0].cpu().data.numpy())\n",
    "\n",
    "    print('saving images...')\n",
    "    print(type(allShifts), np.shape(allShifts))\n",
    "    save_image(torch.Tensor(np.asarray(allShifts)), join(exDir,'shiftImages.png'), nrow=16)\n",
    "    print(robustnessMap)\n",
    "\n",
    "    print('save maps as numpy array...')\n",
    "    np.save(join(exDir, 'classMap.npy'), classMap.numpy())\n",
    "    np.save(join(exDir, 'shiftMap.npy'), robustnessMap.numpy())\n",
    "\n",
    "    # plot shift robustenss map\n",
    "    fig0 = plt.figure()\n",
    "    print(robustnessMap.min(), robustnessMap.max(), robustnessMap.size())\n",
    "    f.write('\\nrobustness min: %0.5f, max: %0.5f' % (robustnessMap.min(), robustnessMap.max()))\n",
    "    plt.imshow(robustnessMap.numpy(), extent=[-maxShift, maxShift, -maxShift, maxShift], vmin=0.0, vmax=1.0)\n",
    "    plt.xlabel('DX')\n",
    "    plt.ylabel('DY')\n",
    "    plt.title('Robustness to shifts in x and y')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(join(exDir, 'ShiftRobustness.png'))\n",
    "\n",
    "    #Plot shift robusteness classification map\n",
    "    fig1 = plt.figure()\n",
    "    f.write('\\nshift robustenss accuracy min: %0.5f, max: %0.5f' % (classMap.min(), classMap.max()))\n",
    "    f.write('\\nAccuray Volume (sum of elements in accuracy shift map): %0.5f' % (np.clip(classMap.numpy(), 0.5, 1).sum()))\n",
    "    plt.imshow(classMap.numpy(), extent=[-maxShift, maxShift, -maxShift, maxShift], vmin=0.5, vmax=1.0)\n",
    "    plt.xlabel('DX')\n",
    "    plt.ylabel('DY')\n",
    "    plt.title('Classiciation Robustness to shifts in x and y')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(join(exDir, 'ClassificationShiftRobustness.png'))\n",
    "\n",
    "    ## Compare histograms for enc, z_samples and encCorr\n",
    "    fig2 = plt.figure()\n",
    "    # nEnc, bEnc, _ = plt.hist(enc00.cpu().data.numpy().flatten(), 100, normed=True)\n",
    "    nEnc, bEnc, _ = plt.hist(enc00.cpu().data.numpy().flatten(), 100, density=True, stacked=True)\n",
    "    xcorr = dae.corrupt(x)\n",
    "    encCorr = dae.encode(xcorr)\n",
    "    # nEncCorr, bEncCorr, _ = plt.hist(encCorr.cpu().data.numpy().flatten(), 100, normed=True)\n",
    "    # nNorm, bNorm, _ = plt.hist(dae.sample_z(10000).cpu().data.numpy().flatten(), 100, normed=True)\n",
    "    nEncCorr, bEncCorr, _ = plt.hist(encCorr.cpu().data.numpy().flatten(), 100, density=True, stacked=True)\n",
    "    nNorm, bNorm, _ = plt.hist(dae.sample_z(10000).cpu().data.numpy().flatten(), 100, density=True, stacked=True)\n",
    "    fig3 = plt.figure()\n",
    "    plt.plot(bEnc[1:], nEnc, label='encoding')\n",
    "    plt.plot(bEncCorr[1:], nEncCorr, label='corrupted encoding')\n",
    "    plt.plot(bNorm[1:], nNorm, label='Normal')\n",
    "    plt.title('Comparing Encodings')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('pdf')\n",
    "    plt.legend()\n",
    "    plt.savefig(join(exDir, 'HisEnc.png'))\n",
    "\n",
    "    #save all histograms:\n",
    "    np.save(join(exDir, 'HistEnc.npy'), [nEnc,bEnc])\n",
    "    np.save(join(exDir), 'HistEncCorr.npy', [nEncCorr, bEncCorr])\n",
    "    np.save(join(exDir), 'prior.npy', [nNorm, bNorm])\n",
    "\n",
    "    #sampling\n",
    "    print('sampling...')\n",
    "    sampleDir = join(exDir,'FinalSamples')\n",
    "    try:\n",
    "        os.mkdir(sampleDir)\n",
    "    except OSError: print('file alread exists')\n",
    "    dae.sample_x(opts.M, sampleDir)\n",
    "\n",
    "    ## classification test score\n",
    "    if svm is not None:\n",
    "        'Do classification'\n",
    "        testScore = 0\n",
    "        for i, data in enumerate(testLoader):\n",
    "            x, y = prep_data(data, useCUDA=svm.useCUDA)\n",
    "            score = svm_score(svm, y, x=x, dae=dae) \n",
    "            testScore+=score\n",
    "    testScore /= (i+1)\n",
    "    # f.write('\\nSVM classification (test) score:'+str(testScore.data[0]))\n",
    "    f.write('\\nSVM classification (test) score:'+str(testScore.detach().cpu().numpy()))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_svm(dae, svm, trainLoader, testLoader, exDir, lr):\n",
    "    '''\n",
    "    Data y is [0,1]\n",
    "    For training SVM must be -1, 1\n",
    "    To eval data put back to [0,1]\n",
    "\n",
    "    To get loss use [-1,1] for train and test\n",
    "    To get score use [0,1] for train and test\n",
    "    '''\n",
    "    print('training svm...')\n",
    "    dae.eval()\n",
    "    optimSVM = optim.SGD(svm.parameters(), lr=lr) #optimizer  \n",
    "\n",
    "    f = open(join(exDir, 'svmOpts.txt'), 'w')\n",
    "    f.write('smvLR: %0.5f\\nc: %0.5f\\n' % (lr, svm.c))\n",
    "    f.close() \n",
    "\n",
    "\n",
    "    svmLoss = {'train':[], 'test':[]}\n",
    "    for epoch in range(opts.maxEpochs):\n",
    "        epochLoss_svm=0\n",
    "        svm.train()\n",
    "        T = time()\n",
    "        for i, data in enumerate(trainLoader):\n",
    "            x, y = prep_data(data, useCUDA=svm.useCUDA)  #prep data as a var\n",
    "            inputs = dae.encode(x)  #get encodings as input\n",
    "            output = svm.forward(inputs)  #get output\n",
    "            loss = svm.loss(output, y * 2 - 1)  #calc loss \n",
    "            optimSVM.zero_grad()  #zero grad\n",
    "            loss.backward()  #backwards\n",
    "            optimSVM.step()  #step\n",
    "            # epochLoss_svm+=loss.data[0]\n",
    "            epochLoss_svm+=loss.detach().cpu().numpy()\n",
    "            if i%100 == 0:\n",
    "                print('[%d, %i] loss: %0.5f, time: %0.3f' % (epoch, i, epochLoss_svm/(i+1), time() - T))\n",
    "        svm.save_params(exDir)\n",
    "        svmLoss['train'].append(epochLoss_svm/(i+1))\n",
    "\n",
    "        #test loss:\n",
    "        svm.eval()\n",
    "        xTest, yTest = prep_data(iter(testLoader).next(), useCUDA=svm.useCUDA)\n",
    "        testInputs = dae.encode(xTest)\n",
    "        testOutputs = svm.forward(testInputs)\n",
    "        testLoss = svm.loss(testOutputs, yTest * 2 - 1)\n",
    "        # svmLoss['test'].append(testLoss.data[0])\n",
    "        svmLoss['test'].append(testLoss.detach().cpu().numpy())\n",
    "\n",
    "        if epoch > 1:\n",
    "            plot_losses(svmLoss, exDir=exDir, epochs=epoch+1, title='SVM_loss')\n",
    "\n",
    "        #Do classification\n",
    "        testScore = svm.binary_class_score(testOutputs, yTest) #has threshold as zero for testOutputs in [-1,1]\n",
    "        trainScore = svm.binary_class_score(output, y) #has threshold as zero for output in [-1,1]\n",
    "        f = open(join(exDir, 'svm.txt'), 'w')\n",
    "        # f.write('trainScore: %f \\ntestScore: %f ' \\\n",
    "        #  % (trainScore.mean().data[0], testScore.mean().data[0]))\n",
    "        f.write('trainScore: %f \\ntestScore: %f ' \\\n",
    "         % (trainScore.mean().detach().cpu().numpy(), testScore.mean().detach().cpu().numpy()))\n",
    "        f.close()\n",
    "\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7bd81-8d0a-46d2-9141-3c0b6aac5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 512 # 512\n",
    "stride_len = 20 # 100\n",
    "# act_list = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
    "act_list = [1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ad157-4fbf-482a-8ab2-297d95a10880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opts = get_args()\n",
    "X=[]\n",
    "user_labels=[]\n",
    "act_labels=[]\n",
    "\n",
    "# columns for IMU data\n",
    "imu_locs = [4,5,6, 10,11,12, 13,14,15, \n",
    "            21,22,23, 27,28,29, 30,31,32, \n",
    "            38,39,40, 44,45,46, 47,48,49\n",
    "           ] \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "for uid in np.arange(1,10):\n",
    "    path = '../../../PAMAP2_Dataset/Protocol/subject10' + str(uid) + '.dat'\n",
    "    df = pd.read_table(path, sep=' ', header=None)\n",
    "    act_imu_filter = df.iloc[:, imu_locs] \n",
    "\n",
    "\n",
    "    for act_id in act_list:\n",
    "        act_filter =  act_imu_filter[df.iloc[:, 1] == act_id]\n",
    "        act_data = act_filter.to_numpy()\n",
    "        if act_data.shape[0] > 0:      \n",
    "            # scaler = StandardScaler()\n",
    "            # scaler = MinMaxScaler()\n",
    "            if uid==1 and act_id == act_list[0]:\n",
    "                scaler.fit(act_data)\n",
    "            act_data = scaler.transform(act_data)\n",
    "            \n",
    "        act_data = np.transpose(act_data)\n",
    "\n",
    "        # sliding window segmentation\n",
    "        start_idx = 0\n",
    "        while start_idx + window_len < act_data.shape[1]:\n",
    "            window_data = act_data[:, start_idx:start_idx+window_len]\n",
    "            downsamp_data = window_data[:, ::3] # downsample from 100hz to 33.3hz\n",
    "            downsamp_data = np.nan_to_num(downsamp_data) # remove nan\n",
    "\n",
    "            X.append(downsamp_data)\n",
    "            user_labels.append(uid)\n",
    "            act_labels.append(act_id)\n",
    "            start_idx = start_idx + stride_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0585e16-8096-445c-9885-1835439ff57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).astype('float32')\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2]) # convert list to numpy array\n",
    "act_labels = np.array(act_labels).astype('float32')\n",
    "act_labels = act_labels.reshape(act_labels.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1850291-52b6-480f-b8b6-3685a4dcb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(act_labels.shape)\n",
    "# plt.imshow(X[1])\n",
    "# plt.show()\n",
    "# plt.pause(1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c2596-4924-4a23-bdfc-54d26a0f1bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed668882-cf82-402e-b13e-6a2a41a29a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__=='__main__':\n",
    "# opts = get_args()\n",
    "opts = Args()\n",
    "\n",
    "# #Load data\n",
    "# print('Prepare data loaders...')\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.RandomHorizontalFlip()])\n",
    "# trainDataset = CELEBA(root=opts.root, train=True, transform=transforms.ToTensor())\n",
    "# trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=opts.batchSize, shuffle=True)\n",
    "\n",
    "# testDataset = CELEBA(root=opts.root, train=False, transform=transforms.ToTensor())\n",
    "# testLoader = torch.utils.data.DataLoader(testDataset, batch_size=opts.batchSize, shuffle=False)\n",
    "# print('Data loaders ready.')\n",
    "\n",
    "print('Prepare data loaders...')\n",
    "\n",
    "dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(act_labels))\n",
    "\n",
    "# Train/Test dataset split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "trainDataset, testDataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset,\n",
    "    batch_size=opts.batchSize, shuffle=True) \n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(testDataset,\n",
    "    batch_size=opts.batchSize, shuffle=False)\n",
    "print('Data loaders ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e2356-5881-4f76-92ac-6794d9c049eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.std(trainDataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81642ac6-0acd-4b12-b6b9-650369c69df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import binary_cross_entropy as bce\n",
    "# from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d0cda-b268-43bd-9ec5-46f35a9f3f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776f577-918e-4e6e-8606-4b4870288fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE(nn.Module):\n",
    "\n",
    "    def __init__(self, nz, imSize, fSize=2, sigma=0.1, multimodalZ=False):  #sigma is the corruption level\n",
    "        super(DAE, self).__init__()\n",
    "        #define layers here\n",
    "\n",
    "        self.fSize = fSize\n",
    "        self.nz = nz\n",
    "        self.imSize = imSize\n",
    "        self.sigma = sigma\n",
    "        self.multimodalZ = multimodalZ\n",
    "\n",
    "        inSize = int(imSize / ( 2 ** 4))\n",
    "        self.inSize = inSize\n",
    "\n",
    "        if multimodalZ:\n",
    "            NZ = 2\n",
    "        else:\n",
    "            NZ = nz\n",
    "        self.enc1 = nn.Conv2d(1, fSize, 5, stride=2, padding=2)\n",
    "        # self.enc1 = nn.Conv2d(1, fSize, (X.shape[2], X.shape[3]), stride=2, padding=2)\n",
    "        # self.enc1 = nn.Conv2d(3, fSize, 5, stride=2, padding=2)\n",
    "        self.enc2 = nn.Conv2d(fSize, fSize * 2, 5, stride=2, padding=2)\n",
    "        self.enc3 = nn.Conv2d(fSize * 2, fSize * 4, 5, stride=2, padding=2)\n",
    "        self.enc4 = nn.Conv2d(fSize * 4, fSize * 8, 5, stride=2, padding=2)\n",
    "        # self.enc5 = nn.Linear((fSize * 8) * inSize * inSize, NZ)\n",
    "        self.enc5 = nn.Linear(11264, NZ)\n",
    "        \n",
    "        # self.dec1 = nn.Linear(NZ, (fSize * 8) * inSize * inSize)\n",
    "        self.dec1 = nn.Linear(NZ, 11264)\n",
    "        self.dec2 = nn.ConvTranspose2d(fSize * 8, fSize * 4, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(fSize * 4, fSize * 2, 2, stride=2, padding=1, output_padding=1)\n",
    "        self.dec4 = nn.ConvTranspose2d(fSize * 2, fSize, 3, stride=2, padding=1, output_padding=1)\n",
    "        # self.dec5 = nn.ConvTranspose2d(fSize, 3, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec5 = nn.ConvTranspose2d(fSize, 1, 2, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.useCUDA = torch.cuda.is_available()\n",
    "\n",
    "    def norm_prior(self, noSamples=25):\n",
    "        z = torch.randn(noSamples, self.nz)\n",
    "        return z\n",
    "\n",
    "    def multi_prior(self, noSamples=25, mode=None):\n",
    "        #make a 2D sqrt(nz)-by-sqrt(nz) grid of gaussians\n",
    "        num = np.sqrt(self.nz) #no of modes in x and y\n",
    "        STD = 1.0\n",
    "        modes = np.arange(-num,num)\n",
    "        p = np.random.uniform(0, num,(noSamples*2))\n",
    "\n",
    "        if mode is None:\n",
    "            mu = modes[np.floor(2 * p).astype(int)]\n",
    "        else:\n",
    "            mu = modes[np.ones((noSamples, 2), dtype=int) * int(mode)]\n",
    "\n",
    "        z = torch.Tensor(mu).view(-1,2) + STD * torch.randn(noSamples, 2)\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        #define the encoder here return mu(x) and sigma(x)\n",
    "        x = self.relu(self.enc1(x))\n",
    "        x = self.relu(self.enc2(x))\n",
    "        x = self.relu(self.enc3(x))\n",
    "        x = self.relu(self.enc4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.enc5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # def corrupt(self, x):\n",
    "    #     noise = self.sigma * torch.randn(x.size()).type_as(x)\n",
    "    #     return x + noise\n",
    "    \n",
    "    def corrupt(self, x):\n",
    "        num_zeros = int(torch.numel(x) * self.sigma)\n",
    "        if self.useCUDA:\n",
    "            mask = torch.ones(torch.numel(x)).cuda()\n",
    "        else:\n",
    "            mask = torch.ones(torch.numel(x))\n",
    "        mask[:num_zeros] = 0\n",
    "        mask = mask[torch.randperm(mask.shape[0])]\n",
    "        mask = mask.reshape(x.shape)#\n",
    "        return torch.mul(x, mask)\n",
    "         \n",
    "    def sample_z(self, noSamples=25, mode=None):\n",
    "        if not self.multimodalZ:\n",
    "            z = self.norm_prior(noSamples=noSamples)\n",
    "        else:\n",
    "            z = self.multi_prior(noSamples=noSamples, mode=mode)\n",
    "        if self.useCUDA:\n",
    "            return z.cuda()\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        #define the decoder here\n",
    "        # print('z')\n",
    "        z = self.relu(self.dec1(z))\n",
    "        # z = z.view(z.size(0), -1, self.inSize, self.inSize)\n",
    "        z = z.view(z.size(0), -1, 2, 11)\n",
    "        z = self.relu(self.dec2(z))\n",
    "        z = self.relu(self.dec3(z))\n",
    "        z = self.relu(self.dec4(z))\n",
    "        z = torch.sigmoid(self.dec5(z))\n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the outputs needed for training\n",
    "        x_corr = self.corrupt(x)\n",
    "        z = self.encode(x_corr)\n",
    "        return z, self.decode(z)\n",
    "\n",
    "    def rec_loss(self, rec_x, x, loss='BCE'):\n",
    "        if loss == 'BCE':\n",
    "            return torch.mean(bce(rec_x, x, size_average=True))  #not averaged over mini-batch if size_average=FALSE and is averaged if =True \n",
    "        elif loss == 'MSE':\n",
    "            return torch.mean(F.mse_loss(rec_x, x, size_average=True))\n",
    "        else:\n",
    "            print('unknown loss:'+loss)\n",
    "\n",
    "    def save_params(self, exDir):\n",
    "        print('saving params...')\n",
    "        torch.save(self.state_dict(), join(exDir, 'dae_params'))\n",
    "\n",
    "    def load_params(self, exDir):\n",
    "        print('loading params...')\n",
    "        self.load_state_dict(torch.load(join(exDir, 'dae_params')))\n",
    "\n",
    "    def sample_x(self, M, exDir, z=None):\n",
    "        if z == None:\n",
    "            z = self.sample_z(noSamples=25)\n",
    "        if not self.multimodalZ:\n",
    "            x_i = self.decode(z)\n",
    "            save_image(x_i.data, join(exDir, 'samples0.png'))\n",
    "            for i in range(M):\n",
    "                z_i, x_i = self.forward(x_i) #corruption already in there!\n",
    "                save_image(x_i.data, join(exDir, 'samples'+str(i+1)+'.png'))\n",
    "        else:\n",
    "            #show samples from a few modes\n",
    "            maxModes = min(self.nz, 5)  #show at most 5 modes\n",
    "            for mode in range(maxModes):\n",
    "                z = self.sample_z(noSamples=25, mode=mode)\n",
    "                x_i = self.decode(z)\n",
    "                save_image(x_i.data, join(exDir, 'mode'+str(mode)+'_samples.png'))\n",
    "                for i in range(M):\n",
    "                    z_i, x_i = self.forward(x_i) #corruption already in there!\n",
    "                    save_image(x_i.data, join(exDir, 'mode'+str(mode)+'_samples'+str(i+1)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d872a-3563-423a-b45b-d07cd58a3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "dae = DAE(nz=opts.nz, imSize=64, fSize=opts.fSize, sigma=opts.sigma, multimodalZ=opts.multimodalZ) #sigma=level of corruption\n",
    "dis, NZ = build_dis(dae=dae, multimodalZ=opts.multimodalZ)\n",
    "svm = LINEAR_SVM(nz=NZ, c=opts.c) #model\n",
    "\n",
    "if dae.useCUDA:\n",
    "    torch.cuda.set_device(opts.gpuNo)\n",
    "    print('using gpu:', torch.cuda.current_device())\n",
    "    dae.cuda()\n",
    "    dis.cuda()\n",
    "    svm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558be4d-7d92-40e7-905a-1e753fb6d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.loadDAE:  #should load DAE if in eval mode\n",
    "    print('loading DAE...')\n",
    "    dae.load_params(opts.load_DAE_from)\n",
    "\n",
    "if opts.loadSVM:\n",
    "        svm.load_params(opts.load_DAE_from) #use SVM @ same location as DAE [may not be one there]\n",
    "\n",
    "if opts.evalMode & (not opts.loadSVM):  #to train an SVM for eval\n",
    "        svm = train_svm(dae=dae, svm=svm, trainLoader=trainLoader, testLoader=testLoader, exDir=opts.load_DAE_from, lr=opts.svmLR)\n",
    "\n",
    "if opts.evalMode:\n",
    "    assert opts.loadDAE == True\n",
    "    eval_mode(dae, opts.load_DAE_from, opts.M, testLoader, svm=svm)\n",
    "    exit()\n",
    "else:\n",
    "    #Create a folder for this experiment\n",
    "    exDir = make_new_folder(opts.outDir)\n",
    "    print('Outputs will be saved to:',exDir)\n",
    "    save_input_args(exDir, opts)  #save training opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890c85c-dc7d-4a8d-8e9e-f5b3381818b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95820977-2ec2-4798-9c80-203fef0a5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create optimizers\n",
    "optimDAE = optim.RMSprop(dae.parameters(), lr = opts.lr)\n",
    "optimDIS = optim.RMSprop(dis.parameters(), lr = opts.lr, momentum=opts.momentum)\n",
    "\n",
    "#Keeping track of training\n",
    "losses = {'enc': [], 'rec': [], 'dis':[], 'test rec':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e100a7-9395-4ffc-81db-41a6bda645ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTest, yTest = prep_data(iter(testLoader).next(), useCUDA=dae.useCUDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4a39d-f18a-4d6c-b89e-ca658d409269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.round(torch.randn(xTest.size()).type_as(xTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10136f3b-34d1-4e9f-b397-daf684a13d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randint(0, 2, xTest.size()).type_as(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216d94a-22b7-44e8-b698-8fe90c05d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6b6d1-416c-4843-9fb7-d9535086a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_image(dae.corrupt(xTest.data), join(exDir, 'corrupted.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31356244-f801-492b-9e88-fcbb1e32592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.min(xTest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdf3f7-89a4-4e5c-ba2c-a3e58b860170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start training\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for e in range(opts.maxEpochs):\n",
    "\n",
    "        epochEncLoss=0\n",
    "        epochRecLoss=0\n",
    "        epochDisLoss=0\n",
    "\n",
    "        for i, data in enumerate(trainLoader):\n",
    "            # print(i)\n",
    "\n",
    "            T = time()\n",
    "\n",
    "            dae.train()\n",
    "            dis.train()\n",
    "\n",
    "            x, y = prep_data(data, useCUDA=dae.useCUDA)\n",
    "            # print(x.shape)\n",
    "            \n",
    "            # get outputs\n",
    "            zFake, xRec = dae.forward(x)\n",
    "\n",
    "            # clac losses\n",
    "            recLoss = dae.rec_loss(xRec, x, loss=opts.loss)  #loss='BCE' or 'MSE'\n",
    "            \n",
    "            optimDAE.zero_grad()\n",
    "            recLoss.backward()\n",
    "            optimDAE.step()            \n",
    "            \n",
    "            \n",
    "            zFake, xRec = dae.forward(x)    \n",
    "            \n",
    "            disLoss = dis.dis_loss(zFake)\n",
    "\n",
    "\n",
    "            \n",
    "            #do updates\n",
    "\n",
    "            optimDIS.zero_grad()\n",
    "            disLoss.backward()\n",
    "            optimDIS.step()\n",
    "            \n",
    "            encLoss = dis.gen_loss(zFake)\n",
    "            \n",
    "            optimDAE.zero_grad()\n",
    "            encLoss.backward()\n",
    "            optimDAE.step()                   \n",
    "                        \n",
    "            # encLoss = dis.gen_loss(zFake)\n",
    "            # daeLoss = recLoss + opts.alpha * encLoss\n",
    "\n",
    "            # print(recLoss)\n",
    "            # print(encLoss)\n",
    "            # print(disLoss)            \n",
    "            \n",
    "\n",
    "            \n",
    "            # print(encLoss.data)\n",
    "\n",
    "            # storing losses for plotting later\n",
    "            # epochEncLoss+=encLoss.data[0]\n",
    "            # epochRecLoss+=recLoss.data[0]\n",
    "            # epochDisLoss+=disLoss.data[0]\n",
    "            epochEncLoss+=encLoss.detach().cpu().numpy()\n",
    "            epochRecLoss+=recLoss.detach().cpu().numpy()\n",
    "            epochDisLoss+=disLoss.detach().cpu().numpy()\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                # print('[%d, %d] enc: %0.5f, rec: %0.5f, dis: %0.5f, time: %0.3f' % (e, i, encLoss.data[0], recLoss.data[0], disLoss.data[0], time() - T))\n",
    "                print('[%d, %d] enc: %0.5f, rec: %0.5f, dis: %0.5f, time: %0.3f' % (e, i, encLoss.detach().cpu().numpy(), recLoss.detach().cpu().numpy(), disLoss.detach().cpu().numpy(), time() - T))\n",
    "\n",
    "        # storing losses for plotting later\n",
    "        losses['enc'].append(epochEncLoss/i)\n",
    "        losses['rec'].append(epochRecLoss/i)\n",
    "        losses['dis'].append(epochDisLoss/i)\n",
    "\n",
    "        #### Test\n",
    "        dae.eval()\n",
    "        dis.eval()\n",
    "\n",
    "        #get test outuputs and losses\n",
    "        xTest, yTest = prep_data(iter(testLoader).next(), useCUDA=dae.useCUDA)\n",
    "        zTest, recTest = dae.forward(xTest)  #N.B. corruption in here\n",
    "        recLossTest = dae.rec_loss(recTest, xTest)\n",
    "\n",
    "        #Plot losses\n",
    "        # losses['test rec'].append(recLossTest.data[0])\n",
    "        losses['test rec'].append(recLossTest.detach().cpu().numpy())\n",
    "\n",
    "        if e > 0: #only one point for test rec otherwise\n",
    "            plot_losses(losses, exDir, epochs=e+1)\n",
    "            plot_norm_losses(losses, exDir)\n",
    "\n",
    "        #save parameters\n",
    "        dae.save_params(exDir)\n",
    "        dis.save_params(exDir)\n",
    "\n",
    "        #Save images of original and rec\n",
    "        save_image(xTest.data, join(exDir, 'original.png'))\n",
    "        save_image(recTest.data, join(exDir, 'rec.png'))\n",
    "\n",
    "        #Save samples\n",
    "        sampleDir = join(exDir,'epoch_'+str(e))\n",
    "        os.mkdir(sampleDir)\n",
    "        print('sample dir:', sampleDir)\n",
    "        dae.sample_x(opts.M, sampleDir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b71b6-3e47-42de-8808-a3984d465343",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not opts.evalMode:\n",
    "    eval_mode(dae=dae, exDir=exDir, M=20, testLoader=testLoader, svm=svm)\n",
    "    svm = train_svm(dae=dae, svm=svm, trainLoader=trainLoader, testLoader=testLoader, exDir=exDir, lr=opts.svmLR)\n",
    "\n",
    "\n",
    "#Train a linear-SVM classifier on the enocdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf33d8-a928-46eb-b386-29e1fb273051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe7b1a-b6aa-4f1a-b653-8722cd2d6852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7449835-8ce8-45d2-8c05-17e21019dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
