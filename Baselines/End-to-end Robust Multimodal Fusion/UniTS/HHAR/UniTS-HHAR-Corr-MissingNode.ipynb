{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8d2ed-260b-40da-8e00-4f336ad95266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from model import *\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8e0b2-4bb9-4d7b-8903-3ec79224fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.config = 'default'\n",
    "        self.dataset = 'HHAR'\n",
    "        self.model = 'UniTS'\n",
    "        self.log = 'log'\n",
    "        self.exp = ''\n",
    "        self.ratio = 0.2\n",
    "        self.epochs = 50\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 64\n",
    "        self.save = True #'BCE'\n",
    "        self.test_only = False\n",
    "        self.input_size = 100 # 24 #256    \n",
    "        self.input_channel = 6 #113 #45\n",
    "        self.hheads = 9\n",
    "        self.SENSOR_AXIS = 3\n",
    "        self.corr='Gaussian' # Gaussian, ConsecutiveZeros, Both, MissingNode\n",
    "        # self.sigma = [0.1, 20, 50]\n",
    "        self.sigma = [0.1]\n",
    "        self.num_labels=6\n",
    "        self.n_gpu = 0\n",
    "        self.random_seed = 0\n",
    "        \n",
    "args = Args()\n",
    "\n",
    "args.log_path = os.path.join(args.log, args.dataset)\n",
    "if not os.path.exists(args.log_path):\n",
    "    os.mkdir(args.log_path)\n",
    "args.model_save_path = os.path.join(args.log_path, args.model + '_'+ args.config +'_'+args.corr+'_'+str(args.sigma[0])+'_' + str(args.random_seed) + '.pt')\n",
    "config = read_config(args.config + '.yaml')\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.n_gpu) \n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(0 if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25b3a3-c0b5-4e61-903d-8bb57828010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = set_up_logging(args, config)\n",
    "args.log = log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5954663b-b379-4faf-99be-e0f70b57986d",
   "metadata": {},
   "source": [
    "## Our pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c399ec4-5672-4dc2-9a54-453c139afbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_labels_txt = ['bike', 'sit', 'std', 'walk', 'stair_up','stair_dwn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ca6f3-7e63-40b1-a6e9-7083044fc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a38b35-a013-4b70-a003-7b5b3343ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../../../../Dataset/'\n",
    "X_total = np.load(folder_path+'hhar_time_X.npy').astype('float32')\n",
    "y_total = np.load(folder_path+'hhar_time_y.npy').astype(int)\n",
    "\n",
    "X_total=np.nan_to_num(X_total)\n",
    "for i in range(X_total.shape[1]):\n",
    "    ch_data = X_total[:,i,:] # the data of channel id\n",
    "    scaler = MinMaxScaler()\n",
    "    ch_data = scaler.fit_transform(ch_data) # scale the data in this channel to [0,1]\n",
    "    X_total[:,i,:] = ch_data # assign normalized data to normalized_X    \n",
    "    \n",
    "X_total = np.transpose(X_total, (0,2,1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=args.random_seed)\n",
    "\n",
    "ytrain = np.argmax(y_train, axis=1)\n",
    "ytest = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bc813-0034-474b-b0f2-27d284f2c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.window_list)\n",
    "print(config.stride_list)\n",
    "print(config.k_list)\n",
    "print(config.layer_num)\n",
    "print(config.hidden_channel)\n",
    "# Adjust model parameters based on our preprocessing\n",
    "config.window_list = [7, 16, 32, 48, 64, 80, 96]\n",
    "config.stride_list = [3, 8, 16, 24, 32, 40, 48]\n",
    "config.k_list = [3, 8, 16, 24, 24, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58b696-1cfe-48d4-a3ca-077683206d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model == 'UniTS':\n",
    "    model = UniTS(input_size = args.input_size, sensor_num = args.input_channel, layer_num = config.layer_num,\n",
    "    window_list = config.window_list, stride_list = config.stride_list, k_list = config.k_list,\n",
    "    out_dim = args.num_labels, hidden_channel = config.hidden_channel).cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "log('Total parameters: ' + str(total_params))\n",
    "\n",
    "if args.test_only:\n",
    "    if os.path.exists(args.model_save_path):\n",
    "        model.load_state_dict(torch.load(args.model_save_path))\n",
    "        test(model, xtest, ytest)\n",
    "    else:\n",
    "        log(\"Model state dict not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1061ed6-e1f4-4bd6-a5ca-2371cb921576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt(args, x):\n",
    "    if args.corr=='Gaussian':\n",
    "        noise = args.sigma[0] * torch.randn(x.size()).type_as(x)\n",
    "        return x + noise\n",
    "    # lambdas reuse the sigma variable, unpack\n",
    "    if args.corr == 'ConsecutiveZeros':\n",
    "        lambda_corr = args.sigma[0] # lambda for missing data period\n",
    "        lambda_norm = args.sigma[1] # lambda for normal data periodß\n",
    "\n",
    "        mask = torch.ones_like(x)\n",
    "        for sample_id in range(mask.shape[0]):\n",
    "            for ch_id in range(mask.shape[2]):  \n",
    "                ptr = 0\n",
    "                is_corrupted = False\n",
    "                while ptr < mask.shape[1]:\n",
    "                    if is_corrupted:\n",
    "                        corr_duration = int(np.random.exponential(scale=lambda_corr))\n",
    "                        mask[sample_id, ptr:min(mask.shape[1], ptr + corr_duration), ch_id] = 0\n",
    "                        ptr = min(mask.shape[1], ptr + corr_duration)\n",
    "                        is_corrupted = False\n",
    "                    else:\n",
    "                        norm_duration = int(np.random.exponential(scale=lambda_norm))\n",
    "                        ptr = min(mask.shape[1], ptr + norm_duration)\n",
    "                        is_corrupted = True\n",
    "        x = torch.mul(x, mask)\n",
    "        return x\n",
    "    if args.corr == 'Both':\n",
    "        noise = args.sigma[0] * torch.randn(x.size()).type_as(x)\n",
    "        x = x + noise\n",
    "        # lambdas reuse the sigma variable, unpack\n",
    "        lambda_corr = args.sigma[1] # lambda for missing data period\n",
    "        lambda_norm = args.sigma[2] # lambda for normal data periodß\n",
    "\n",
    "        mask = torch.ones_like(x)\n",
    "        for sample_id in range(mask.shape[0]):\n",
    "            for ch_id in range(mask.shape[2]):  \n",
    "                ptr = 0\n",
    "                is_corrupted = False\n",
    "                while ptr < mask.shape[1]:\n",
    "                    if is_corrupted:\n",
    "                        corr_duration = int(np.random.exponential(scale=lambda_corr))\n",
    "                        mask[sample_id, ptr:min(mask.shape[1], ptr + corr_duration), ch_id] = 0\n",
    "                        ptr = min(mask.shape[1], ptr + corr_duration)\n",
    "                        is_corrupted = False\n",
    "                    else:\n",
    "                        norm_duration = int(np.random.exponential(scale=lambda_norm))\n",
    "                        ptr = min(mask.shape[1], ptr + norm_duration)\n",
    "                        is_corrupted = True\n",
    "        x = torch.mul(x, mask)\n",
    "        return x\n",
    "    \n",
    "    elif args.corr == 'MissingNode':\n",
    "        # lambdas reuse the sigma variable, unpack\n",
    "        if args.dataset == 'PAMAP2':\n",
    "            imu_chs = [[0,1,2, 3,4,5, 6,7,8], [9,10,11, 12,13,14, 15,16,17], [18,19,20, 21,22,23, 24,25,26]] # [imu1, imu2, imu3]\n",
    "        elif args.dataset == 'OPPO':\n",
    "            imu_chs = [\n",
    "                [0,1,2], [3,4,5], [6,7,8], [9,10,11], [12,13,14], [15,16,17], [18,19,20], [21,22,23], [24,25,26], [27,28,29], [30,31,32], [33,34,35], # 12 accs\n",
    "                [36,37,38, 39,40,41, 42,43,44], [45,46,47, 48,49,50, 51,52,53], [54,55,56, 57,58,59, 60,61,62], [63,64,65, 66,67,68, 69,70,71], [72,73,74, 75,76,77, 78,79,80], # 5 IMUs\n",
    "                [81,82,83, 84,85,86, 87,88,89, 90,91,92, 93,94,95, 96,], [97,98,99, 100,101,102, 103,104,105, 106,107,108, 109,110,111, 112] # 2 shoe sensors\n",
    "            ]\n",
    "        elif args.dataset == 'HHAR':\n",
    "            imu_chs = [[0,1,2], [3,4,5]] # [imu1, imu2, imu3]\n",
    "\n",
    "        lambda_corr = args.sigma[0] # lambda for missing data period\n",
    "        lambda_norm = args.sigma[1] # lambda for normal data periodß\n",
    "\n",
    "        mask = torch.ones_like(x)\n",
    "        for sample_id in range(mask.shape[0]):\n",
    "            if args.dataset == 'PAMAP2':\n",
    "                node_num = 3\n",
    "            elif args.dataset == 'OPPO':\n",
    "                node_num = 19 # 5+12+2\n",
    "            elif args.dataset == 'HHAR':\n",
    "                node_num = 2\n",
    "\n",
    "            for imu_id in range(node_num):  \n",
    "                ptr = 0\n",
    "                is_corrupted = False\n",
    "                while ptr < mask.shape[1]:\n",
    "                    if is_corrupted:\n",
    "                        corr_duration = int(np.random.exponential(scale=lambda_corr))\n",
    "                        mask[sample_id, ptr:min(mask.shape[1], ptr + corr_duration), imu_chs[imu_id]] = 0\n",
    "                        ptr = min(mask.shape[1], ptr + corr_duration)\n",
    "                        is_corrupted = False\n",
    "                    else:\n",
    "                        norm_duration = int(np.random.exponential(scale=lambda_norm))\n",
    "                        ptr = min(mask.shape[1], ptr + norm_duration)\n",
    "                        is_corrupted = True\n",
    "        x = torch.mul(x, mask)\n",
    "        return x        \n",
    "    \n",
    "    else:\n",
    "        print(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857711b-bd14-4ec9-b782-f1eceee40a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = torch.from_numpy(X_train)\n",
    "xtest = torch.from_numpy(X_test)\n",
    "ytrain = torch.from_numpy(ytrain)\n",
    "ytest = torch.from_numpy(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1c708-ee12-4518-9c8b-c53ccf92b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = corrupt(args, xtrain)\n",
    "xtest = corrupt(args, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee211e-5b53-4ffe-8cb9-5e028023ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(xtrain, ytrain)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    batch_size=args.batch_size, shuffle=True, drop_last = True)  \n",
    "\n",
    "test_dataset = TensorDataset(xtest, ytest)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "    batch_size=args.batch_size, shuffle=False, drop_last = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398743d4-afe5-4957-a2d2-339a9e706202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "\n",
    "            x = x.to(device)\n",
    "            y_true += list(y.numpy())\n",
    "\n",
    "            out = model(x)\n",
    "            pred = torch.argmax(out, dim = -1)\n",
    "            y_pred += pred.cpu().tolist()\n",
    "\n",
    "    log(\"Accuracy:\\n\" + str(accuracy_score(y_true, y_pred)) +\n",
    "        \"\\nWeighted F1:\\n\" + str(f1_score(y_true, y_pred, labels=list(range(args.num_labels)),average='weighted')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea90dd-d38c-414d-aa80-17c0337aa9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "try:\n",
    "    for ep in range(1, 1+args.epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        log(\"Training epoch : \" + str(ep))\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)          \n",
    "                 \n",
    "            out = model(x)\n",
    "            loss = loss_func(out, y)\n",
    "            epoch_loss += loss.cpu().item()\n",
    "\n",
    "            optimizer.zero_grad()           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        log(\"Training loss : \" + str(epoch_loss / (len(xtrain) / args.batch_size)))\n",
    "        test(model, test_loader)\n",
    "        log(\"----------------------------\")\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Exiting from training early')\n",
    "    test(model, test_loader)\n",
    "# if args.save:\n",
    "#     torch.save(model.state_dict(), args.model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c8c2e-c9a3-43f6-9903-6390cbdd8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), args.model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c332981-62e7-4cc4-9012-68c52d2ea07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eval_Args:\n",
    "    def __init__(self):\n",
    "        self.corr=args.corr\n",
    "        self.sigma=[0.05]\n",
    "        self.dataset=args.dataset\n",
    "eval_args = eval_Args()\n",
    "\n",
    "xtest_eval = torch.from_numpy(X_test)\n",
    "xtest_eval = corrupt(eval_args, xtest_eval)\n",
    "\n",
    "test_dataset_eval = TensorDataset(xtest_eval, ytest)\n",
    "test_loader_eval = torch.utils.data.DataLoader(test_dataset_eval,\n",
    "    batch_size=args.batch_size, shuffle=False, drop_last = True) \n",
    "\n",
    "test(model, test_loader_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
