# Centaur: Robust Multimodal Fusion for Human Activity Recognition
![This is an image](https://img.shields.io/badge/arXiv-2303.04636-darkred) ![This is an image](https://img.shields.io/badge/license-MIT-green)

This repository contains the implementation of the paper entitled "Centaur: Robust Multimodal Fusion for Human Activity Recognition".

## Datasets
The original (not preprocessed) datasets can be found at the following links:

 * PAMAP2 https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring
 * OPPORTUNITY https://archive.ics.uci.edu/ml/datasets/OPPORTUNITY+Activity+Recognition
 * HHAR https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition


## Dependencies
Package       | Version
------------- | -------------
Python3       | Content Cell
PyTorch       | Content Cell
TensorFlow    | Content Cell
scikit-learn  | Content Cell

## Citation
Sanju Xaviar, Xin Yang and Omid Ardakanian. 2023. [Robust Multimodal Fusion for Human Activity Recognition](https://arxiv.org/abs/2303.04636), preprint.
```
@misc{https://doi.org/10.48550/arxiv.2303.04636,
  doi = {10.48550/ARXIV.2303.04636},
  url = {https://arxiv.org/abs/2303.04636}, 
  author = {Xaviar, Sanju and Yang, Xin and Ardakanian, Omid}, 
  keywords = {Machine Learning (cs.LG), Signal Processing (eess.SP), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS:  Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {Robust Multimodal Fusion for Human Activity Recognition},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
```
