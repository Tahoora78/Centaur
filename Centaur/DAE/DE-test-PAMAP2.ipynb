{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c25dc8-18db-4931-8d8f-8a833af4c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from torch.utils.data import TensorDataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix,f1_score\n",
    "from utils.function import *\n",
    "from models.de_PAMAP2 import DAE\n",
    "from models.activity_recognition import *\n",
    "# from models.activity_recognition import get_eval_model\n",
    "from models.dis_z import DIS_Z\n",
    "import copy\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d944f1-784f-4210-9546-deaf048b6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.root = '../data'\n",
    "        self.batchSize = 64\n",
    "        self.maxEpochs = 100\n",
    "        self.nz = 200\n",
    "        self.lr = 1e-4\n",
    "        self.fSize = 64\n",
    "        self.outDir = 'data/experiments/DE_PAMAP'\n",
    "        self.commit = 'eval'\n",
    "        self.alpha = 1.0\n",
    "        # self.sigma = 0.35\n",
    "        self.M = 5\n",
    "        self.loss = 'MSE' #'BCE'\n",
    "        self.loadDAE = False\n",
    "        self.loadSVM = False    \n",
    "        self.load_DAE_from = None\n",
    "        self.evalMode = False\n",
    "        self.comment = ''\n",
    "        self.momentum = 0.1\n",
    "        self.c = 0.01\n",
    "        self.svmLR = 1e-4\n",
    "        self.Ntest = 100\n",
    "        self.gpuNo = 3\n",
    "        self.multimodalZ = False\n",
    "        self.window_len = 512\n",
    "        self.stride_len = 20\n",
    "        self.act_list = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
    "        self.imSize = 64\n",
    "        # self.sigma = [50, 80]\n",
    "        # self.sigma = [0.2, 50, 80]\n",
    "        self.sigma=0.05\n",
    "        # self.cuda_id = 2\n",
    "        self.random_seed = 2\n",
    "        self.train_split = 0.8\n",
    "\n",
    "        #self.corr= 'ZeroMask' # options: Gaussian, ZeroMask, ConsecutiveZeros\n",
    "\n",
    "        # path for corruption \"consecutive interval\"sigma [60,80]\n",
    "        self.dae_model_loc = \"data/experiments/DE_EVAL_PAMAP_Noise/Ex_12/\" \n",
    "\n",
    "\n",
    "        # self.ar_model_loc = \"data/experiments/DAAE1000/Ex_136/ar_params\"\n",
    "        self.ar_model_loc = \"data/pamap2_ConvAttn.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6f4ee-8711-4258-a933-55e1bf8e6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 3\n",
    "\n",
    "if gpu_id>=0:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    cuda_id = \"cuda:\" + str(0)  # cuda:2\n",
    "\n",
    "device = torch.device(cuda_id if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.set_device(cuda_id)\n",
    "    print(\"Current GPU ID:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff755101-ccb9-44fb-9b83-7011f4c7ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dis(args, dae, multimodalZ):\n",
    "    if not multimodalZ:\n",
    "        print('\\n ** USING NORMAL PRIOR **')\n",
    "        prior = dae.norm_prior\n",
    "        NZ = args.nz\n",
    "    else:\n",
    "        print('\\n ** USING MULTIMODAL PRIOR **')\n",
    "        prior = dae.multi_prior\n",
    "        NZ = 2\n",
    "    dis = DIS_Z(nz=NZ, prior=prior)\n",
    "\n",
    "    return dis, NZ\n",
    "\n",
    "\n",
    "def analysis(args, dae, testDataset, X, num_classes):\n",
    "    # Prepare testdata set, drop the last incomplete batch\n",
    "    test_x = torch.zeros(len(testDataset), X.shape[1], X.shape[2], X.shape[3])\n",
    "    test_labels = torch.zeros(len(testDataset), num_classes)\n",
    "    for test_id in range(len(testDataset)):\n",
    "        test_labels[test_id] = testDataset[test_id][1]\n",
    "        test_x[test_id] = testDataset[test_id][0]\n",
    "    print(test_x.shape)\n",
    "\n",
    "    # Corrupt dataset\n",
    "    corr_test_x = dae.corrupt(test_x)\n",
    "    \n",
    "    mean_fill_test = interpolation_meanfilling(corr_test_x)\n",
    "    linear_interp_test = linear_interpolation(corr_test_x)\n",
    "# recon_test -> synthesize the entire dataset\n",
    "    z = dae.encode(corr_test_x)\n",
    "    recon_test = dae.decode(z)\n",
    "\n",
    "    # breakpoint()\n",
    "# reconstruct filling testset (fill missing values only, not valid for noisy data)\n",
    "    recon_fill_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "    np.copyto(recon_fill_test, recon_test.detach().numpy(), where = recon_fill_test==0)\n",
    "    recon_fill_test = torch.from_numpy(recon_fill_test)\n",
    "\n",
    "    raw_test_dataset = TensorDataset(test_x, test_labels)\n",
    "    corr_test_dataset = TensorDataset(corr_test_x, test_labels)\n",
    "    recon_test_dataset = TensorDataset(recon_test, test_labels)\n",
    "    recon_fill_test_dataset = TensorDataset(recon_fill_test, test_labels)\n",
    "    mean_fill_test_dataset = TensorDataset(mean_fill_test, test_labels)\n",
    "    linear_interp_test_dataset = TensorDataset(linear_interp_test, test_labels)\n",
    "\n",
    "    return test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, linear_interp_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset,linear_interp_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac638067-7240-4159-b37b-c2532899ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_meanfilling(corr_test_x):\n",
    "    mean_fill_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "    for i in range(mean_fill_test.shape[0]):\n",
    "        for j in range(mean_fill_test.shape[2]):\n",
    "            if np.count_nonzero(mean_fill_test[i,:,j,:]) == 0:  \n",
    "                ch_mean = 0\n",
    "            else:\n",
    "             #ch_mean = np.sum(mean_fill_test[i][0][j]) / np.count_nonzero(mean_fill_test[i][0][j])\n",
    "                ch_mean = np.sum(mean_fill_test[i,:,j,:]) / np.count_nonzero(mean_fill_test[i,:,j,:])\n",
    "                mean_fill_test[i,:,j,:][mean_fill_test[i,:,j,:] == 0] = ch_mean\n",
    "    mean_fill_test = torch.from_numpy(mean_fill_test)\n",
    "    return mean_fill_test\n",
    "\n",
    "\n",
    "def linear_interpolation(corr_test_x):\n",
    "    linear_interp_test = copy.deepcopy(corr_test_x).detach().cpu().numpy()\n",
    "    # corr_text_x shape is 18944, 1, 27, 171\n",
    "    # expected shape 18944, 171, 27\n",
    "    # linear_interp_test = linear_interp_test.reshape(-1, 171,27)\n",
    "    linear_interp_test = linear_interp_test.reshape(-1, linear_interp_test.shape[3],linear_interp_test.shape[2])\n",
    "    for i in range(linear_interp_test.shape[0]):\n",
    "        for j in range(linear_interp_test.shape[2]):\n",
    "            if np.count_nonzero(linear_interp_test[i,:,j]) == 0: # when all data points in this channel are missing\n",
    "                linear_interp_test[i, :, j] = 0.0\n",
    "            else:\n",
    "                idxs = np.arange(linear_interp_test.shape[1]) # indexes of all the samples\n",
    "                zero_filter = linear_interp_test[i,:,j] == 0 # index filter for zero values\n",
    "                zero_idxs = idxs[zero_filter] # indexes for zero values\n",
    "                non_zero_idxs = idxs[~zero_filter] # xp, indexes for non-zero values\n",
    "                non_zero_vals = linear_interp_test[i, ~zero_filter,j] # fp, non-zero values\n",
    "                interp_vals = np.interp(zero_idxs, non_zero_idxs, non_zero_vals) # interpolated values\n",
    "                linear_interp_test[i,zero_idxs,j] = interp_vals # fill interpolated values to the corrupted signal\n",
    "    linear_interp_test = torch.from_numpy(linear_interp_test)\n",
    "    linear_interp_test = torch.reshape(linear_interp_test, (-1, 1, linear_interp_test.shape[2], linear_interp_test.shape[1]))\n",
    "    return linear_interp_test\n",
    "\n",
    "def evaluate_rmse(corr_test_x, recon_test, recon_fill_test,mean_fill_test,linear_interp_test,test_x):\n",
    "    corr_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), corr_test_x.reshape(corr_test_x.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Corr RMSE:\\n' + str(corr_rms))\n",
    "\n",
    "    recon_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), recon_test.reshape(recon_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Recon RMSE:\\n' + str(recon_rms))\n",
    "\n",
    "    recon_fill_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), recon_fill_test.reshape(recon_fill_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Recon fill RMSE:\\n' + str(recon_fill_rms))\n",
    "\n",
    "    mean_fill_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1).cpu().detach().numpy(), mean_fill_test.reshape(mean_fill_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Mean Fill RMSE:\\n' + str(mean_fill_rms))\n",
    "    \n",
    "    linear_interp_rms = mean_squared_error(test_x.reshape(test_x.shape[0],-1), linear_interp_test.reshape(linear_interp_test.shape[0],-1).cpu().detach().numpy(), squared=False)\n",
    "    print('Linear Interpolation RMSE:\\n' + str(linear_interp_rms))\n",
    "    return \n",
    "#def plot(test_x, corr_test_x, recon_test)\n",
    "\n",
    "def plot(test_x, corr_test_x, recon_test, recon_fill_test,mean_fill_test, linear_interp_test):\n",
    "    plt.imshow(test_x[0][0].detach())\n",
    "    plt.title('Raw Data')\n",
    "   # plt.subplot(4,1,1)\n",
    "    plt.savefig(join(args.dae_model_loc, 'raw.png'))\n",
    "\n",
    "    plt.imshow(corr_test_x[0][0].detach())\n",
    "    plt.title('Corrupted')\n",
    "   # plt.subplot(4,1,2)\n",
    "    plt.savefig(join(args.dae_model_loc, 'corr.png'))\n",
    "\n",
    "    plt.imshow(recon_test[0][0].detach())\n",
    "    plt.title('Reconstructed')\n",
    "   # plt.subplot(4,1,3)\n",
    "    plt.savefig(join(args.dae_model_loc, 'reconstructed.png'))\n",
    "\n",
    "    plt.imshow(recon_fill_test[0][0].detach())\n",
    "    plt.title('Reconstructed fill')\n",
    "   # plt.subplot(4,1,3)\n",
    "    plt.savefig(join(args.dae_model_loc, 'rec_fill.png'))\n",
    "\n",
    "    plt.imshow(mean_fill_test[0][0].detach())\n",
    "    plt.title('Mean Fill')\n",
    "   # plt.subplot(4,1,4)\n",
    "    plt.savefig(join(args.dae_model_loc, 'mean.png'))\n",
    "    \n",
    "    plt.imshow(linear_interp_test[0][0].detach())\n",
    "    plt.title('Linear Interp')\n",
    "   # plt.subplot(4,1,4)\n",
    "    plt.savefig(join(args.dae_model_loc, 'linear.png'))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def test_dae(args, path, trainLoader, testLoader):\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_activity_recognition(args, path, trainLoader, testLoader):\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_combined_accuracy(args, test_loader, sigma):\n",
    "    # device = torch.device(args.gpuNo if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #ar = ActivityRecognitionCNN(len(args.act_list))\n",
    "    # ar = get_eval_model(len(args.act_list), model_path=args.ar_model_loc)\n",
    "    #ar.load_state_dict(torch.load(args.ar_model_loc))\n",
    "    args.n_sensor_channels = 27\n",
    "    args.len_seq = 171\n",
    "    args.num_classes=12\n",
    "    \n",
    "    ar = get_eval_model(n_sensor_channels=args.n_sensor_channels, len_seq=args.len_seq, num_classes=args.num_classes, model_path=args.ar_model_loc) #n_sensor_channels, len_seq, num_classes, model_path\n",
    "    ar = ar.to(device)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_true = []\n",
    "    total_pred = []\n",
    "\n",
    "    with torch.no_grad():   \n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  \n",
    "            # print(images.shape)\n",
    "            \n",
    "            outputs = ar(images)\n",
    "\n",
    "            # _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = torch.argmax(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "            \n",
    "            total_pred = total_pred + predicted.cpu().numpy().tolist()\n",
    "            total_true = total_true + (torch.argmax(labels, dim=1).cpu().numpy().tolist())\n",
    "            \n",
    "    print(f'Test Accuracy:\\n{100.0 * correct / total}')\n",
    "    \n",
    "    # print(\" | \".join(act_labels_txt))\n",
    "    conf_mat = confusion_matrix(y_true = total_true, y_pred = total_pred)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    print(np.array(conf_mat).round(3) * 100)  \n",
    "    f1 = f1_score(y_true = total_true, y_pred = total_pred, average='weighted')\n",
    "    print('F1 score:\\n', f1)\n",
    "    print('')\n",
    "    return \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7439c1-2981-41bd-88b1-1af069b44300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe668ed-2364-4e3f-86c2-da1dc345808d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd61961-f10b-4720-afe9-118da160d3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f846e27-c581-4188-a91f-50a5313dc137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading params...\n",
      "torch.Size([18944, 1, 27, 171])\n",
      "Raw testset:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw testset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m raw_test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(raw_test_dataset,\n\u001b[1;32m     39\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mbatchSize, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mcalculate_combined_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_test_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrupted testset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m corr_test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(corr_test_dataset,\n\u001b[1;32m     44\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mbatchSize, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mcalculate_combined_accuracy\u001b[0;34m(args, test_loader, sigma)\u001b[0m\n\u001b[1;32m    104\u001b[0m args\u001b[38;5;241m.\u001b[39mlen_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m171\u001b[39m\n\u001b[1;32m    105\u001b[0m args\u001b[38;5;241m.\u001b[39mnum_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m\n\u001b[0;32m--> 107\u001b[0m ar \u001b[38;5;241m=\u001b[39m \u001b[43mget_eval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_sensor_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_sensor_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlen_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mar_model_loc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#n_sensor_channels, len_seq, num_classes, model_path\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ar \u001b[38;5;241m=\u001b[39m ar\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    110\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Code/Robust_fusion-project/Baselines/DAAE/DAAE&ActivityRecognition_v8/models/activity_recognition.py:115\u001b[0m, in \u001b[0;36mget_eval_model\u001b[0;34m(n_sensor_channels, len_seq, num_classes, model_path)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_eval_model\u001b[39m(n_sensor_channels, len_seq, num_classes, model_path):\n\u001b[1;32m    114\u001b[0m     model \u001b[38;5;241m=\u001b[39m HAR(n_sensor_channels\u001b[38;5;241m=\u001b[39mn_sensor_channels, len_seq\u001b[38;5;241m=\u001b[39mlen_seq, n_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m--> 115\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    116\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:607\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    606\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 607\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:882\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    881\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 882\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:857\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m data_type, key, location, size \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m--> 857\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:846\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    843\u001b[0m dtype \u001b[38;5;241m=\u001b[39m data_type(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    845\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, size, dtype)\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[0;32m--> 846\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m storage_type(obj\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/_utils.py:79\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/cuda/__init__.py:606\u001b[0m, in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m _lazy_init()\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# We may need to call lazy init again if we are a forked child\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# del _CudaBase.__new__\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_CudaBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    if args.multimodalZ:\n",
    "        args.nz = 2\n",
    "        \n",
    "    args.num_classes=12\n",
    "    random.seed(args.random_seed)\n",
    "    np.random.seed(args.random_seed)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "\n",
    "    # X, labels = prepare_data(args)\n",
    "    # trainDataset, testDataset, trainLoader, testLoader = prepare_dataloaders(args, X, labels)\n",
    "    X_train, X_test, y_train, y_test = prepare_data_PAMAP2(args)\n",
    "    \n",
    "    tail = len(X_test) % args.batchSize\n",
    "    X = torch.from_numpy(X_test[:len(X_test)-tail,:,:])\n",
    "    labels = torch.from_numpy(y_test[:len(X_test)-tail,:])\n",
    "    # X, labels = prepare_data(args)\n",
    "    # trainDataset, testDataset, trainLoader, testLoader = prepare_dataloaders(args, X, labels)\n",
    "    \n",
    "    testDataset = TensorDataset(X, labels)\n",
    "    \n",
    "    \n",
    "    dae = DAE(nz=args.nz, imSize=args.imSize, fSize=args.fSize, sigma=args.sigma, multimodalZ=args.multimodalZ)\n",
    "    dae.load_params(args.dae_model_loc)\n",
    "\n",
    "    # test_dae(args, dae_path, trainLoader, testLoader)\n",
    "    # test_activity_recognition(args, ar_path, trainLoader, testLoader)\n",
    "\n",
    "    #acc = calculate_combined_accuracy(args, testLoader, sigma=0.5)\n",
    "    test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, linear_interp_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset, linear_interp_test_dataset = analysis(args, dae, testDataset, X_test, args.num_classes)\n",
    "    # test_x, corr_test_x, recon_test, recon_fill_test, mean_fill_test, raw_test_dataset, corr_test_dataset, recon_test_dataset, recon_fill_test_dataset, mean_fill_test_dataset = analysis(dae, testDataset, labels)\n",
    "    #corr_test_x, test_x, recon_test, raw_test_dataset, corr_test_dataset, recon_test_dataset = analysis(dae, testDataset, labels)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Raw testset:\")\n",
    "    raw_test_loader = torch.utils.data.DataLoader(raw_test_dataset,\n",
    "    batch_size= args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args, raw_test_loader, sigma=args.sigma)\n",
    "\n",
    "    print(\"Corrupted testset:\")\n",
    "    corr_test_loader = torch.utils.data.DataLoader(corr_test_dataset,\n",
    "    batch_size= args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,corr_test_loader, sigma=args.sigma)\n",
    "\n",
    "    print(\"Reconstructed testset:\")\n",
    "    recon_test_loader = torch.utils.data.DataLoader(recon_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,recon_test_loader, sigma=args.sigma)\n",
    "\n",
    "\n",
    "    print(\"Reconstructed fill testset:\")\n",
    "    recon_fill_test_loader = torch.utils.data.DataLoader(recon_fill_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,recon_fill_test_loader, sigma=args.sigma)\n",
    "\n",
    "\n",
    "    print(\"Mean Fill testset\")\n",
    "    mean_fill_test_loader = torch.utils.data.DataLoader(mean_fill_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,mean_fill_test_loader, sigma=args.sigma)\n",
    "    \n",
    "    print(\"Linear Interpolation testset:\")\n",
    "    linear_interp_test_loader = torch.utils.data.DataLoader(linear_interp_test_dataset,\n",
    "    batch_size=args.batchSize, shuffle=False)\n",
    "    calculate_combined_accuracy(args,linear_interp_test_loader, sigma=args.sigma)\n",
    "\n",
    "    evaluate_rmse(corr_test_x, recon_test,recon_fill_test, mean_fill_test, linear_interp_test,test_x)\n",
    "    plot(test_x, corr_test_x, recon_test,recon_fill_test, mean_fill_test, linear_interp_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38740a4d-b9e5-4f15-8fda-8e1f56a26a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4b8b5-9d28-4f96-8037-1ef363b665ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
